{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing Necessary Libraries"
      ],
      "metadata": {
        "id": "GcMjeUZb974_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThZtqeR-92cq",
        "outputId": "6e501f2e-3d1f-4f61-a491-5b9d82c3c4dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install gensim pandas nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "BFBYAtFBBlVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from gensim import corpora, models\n",
        "import os"
      ],
      "metadata": {
        "id": "fwTsLPZHBkXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading and Processing the Text Data"
      ],
      "metadata": {
        "id": "HOj6iide-AS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "df = pd.read_csv('/content/df_file.csv')\n",
        "\n",
        "# Extracting text data\n",
        "texts = df['Text'].tolist()\n",
        "\n",
        "# Preprocess the texts\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess(text):\n",
        "    # Tokenize and remove stopwords\n",
        "    return [word for word in tokenizer.tokenize(text.lower()) if word not in stop_words and len(word) > 1]\n",
        "\n",
        "processed_texts = [preprocess(text) for text in texts]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlxNjbkY-EVN",
        "outputId": "91cc4e4f-4a04-4b22-eea6-b911851c075c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Dictionary and Corpus"
      ],
      "metadata": {
        "id": "mVHG776f-N_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary representation of the documents.\n",
        "dictionary = corpora.Dictionary(processed_texts)\n",
        "\n",
        "# Filter out extremes to limit the number of features\n",
        "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=100000)\n",
        "\n",
        "# Create a corpus: list of bag-of-words vectors for each document\n",
        "corpus = [dictionary.doc2bow(text) for text in processed_texts]\n"
      ],
      "metadata": {
        "id": "N09EUxAC-MQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the LDA model"
      ],
      "metadata": {
        "id": "MaNVnxsm-RnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model = models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15, random_state=100)\n",
        "\n",
        "# Print the topics\n",
        "topics = lda_model.print_topics(num_words=5)\n",
        "for topic in topics:\n",
        "    print(topic)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlvqeKr7-Sux",
        "outputId": "f4607fdf-2f2d-42a3-8278-24d1d85350e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, '0.009*\"people\" + 0.006*\"new\" + 0.005*\"technology\" + 0.005*\"mobile\" + 0.004*\"one\"')\n",
            "(1, '0.012*\"us\" + 0.009*\"growth\" + 0.008*\"bank\" + 0.007*\"economy\" + 0.006*\"market\"')\n",
            "(2, '0.016*\"mr\" + 0.011*\"government\" + 0.006*\"us\" + 0.006*\"new\" + 0.006*\"eu\"')\n",
            "(3, '0.006*\"mr\" + 0.005*\"one\" + 0.004*\"best\" + 0.004*\"first\" + 0.004*\"time\"')\n",
            "(4, '0.010*\"us\" + 0.009*\"sales\" + 0.007*\"new\" + 0.005*\"company\" + 0.005*\"market\"')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the Trained Model"
      ],
      "metadata": {
        "id": "C2Y2UhAXAnQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory for saving LDA model components\n",
        "base_dir = 'lda_model_files'\n",
        "os.makedirs(base_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
        "\n",
        "# Save the dictionary\n",
        "dictionary.save(os.path.join(base_dir, 'lda_dictionary.dict'))\n",
        "\n",
        "# Save the corpus\n",
        "corpora.MmCorpus.serialize(os.path.join(base_dir, 'lda_corpus.mm'), corpus)\n",
        "\n",
        "# Save the LDA model\n",
        "lda_model.save(os.path.join(base_dir, 'lda_model.lda'))"
      ],
      "metadata": {
        "id": "oQDfnOeGAmiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Identifying Topics with LDA Model"
      ],
      "metadata": {
        "id": "sh00JkUI-3dx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dictionary\n",
        "loaded_dictionary = corpora.Dictionary.load(os.path.join(base_dir, 'lda_dictionary.dict'))\n",
        "\n",
        "# Load the corpus\n",
        "loaded_corpus = corpora.MmCorpus(os.path.join(base_dir, 'lda_corpus.mm'))\n",
        "\n",
        "# Load the LDA model\n",
        "loaded_lda_model = models.LdaModel.load(os.path.join(base_dir, 'lda_model.lda'))"
      ],
      "metadata": {
        "id": "y6wn3ynvBxo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_lda(text):\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    # Tokenize and remove stopwords\n",
        "    return [word for word in tokenizer.tokenize(text.lower()) if word not in stop_words and len(word) > 1]\n",
        "\n",
        "def identify_topics_lda(text, lda_model, dictionary):\n",
        "    # Preprocess the text\n",
        "    processed_text = preprocess_lda(text)\n",
        "    # Transform text into the bag-of-words space\n",
        "    bow_vector = dictionary.doc2bow(processed_text)\n",
        "    # Perform LDA analysis\n",
        "    lda_output = lda_model[bow_vector]\n",
        "    # Sort topics by their contribution\n",
        "    lda_output = sorted(lda_output, key=lambda tup: -1*tup[1])\n",
        "\n",
        "    # Extract and print topics in a more readable format\n",
        "    for topic_number, prob in lda_output:\n",
        "        # Extract the topic\n",
        "        topic = lda_model.show_topic(topic_number, 5)\n",
        "        # Prepare a list of only words (ignore the probabilities)\n",
        "        topic_words = \", \".join([word for word, prop in topic])\n",
        "        print(f\"Topic {topic_number} ({prob:.3f}): {topic_words}\")"
      ],
      "metadata": {
        "id": "zW5sohZn-0Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example usage\n"
      ],
      "metadata": {
        "id": "wEyBXgAD_Pa6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_text =\"to control immigration and asylum and criticised its record on the NHS, telling delegates Labour cannot be trusted on education or crime. A Tory government would sort out the shambles of immigration, put patients before statistics and bring discipline to schools, he said. Michael Howard, who had been due to welcome delegates to the conference on Friday, will address them in a lunchtime speech. His welcome address had to be postponed after he stayed in London to lead the party's opposition to the Prevention of Terrorism Bill in its lengthy progress through Parliament. The bill was finally passed on Friday evening, after more than 30 hours of debate. Mr Howard is likely to defend his party's handling of the bill, which was only passed after the Conservatives accepted Prime Minister Tony Blair's promise that MPs would be able to review it within a year.\"\n",
        "identify_topics_lda(new_text, loaded_lda_model, loaded_dictionary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YixVU8C_OUt",
        "outputId": "f9ab6957-1a2d-430a-e9cb-dcb52a59f47d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 3 (0.876): mr, one, best, first, time\n",
            "Topic 2 (0.116): mr, government, us, new, eu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer to Drive"
      ],
      "metadata": {
        "id": "X32lja3kCQ-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTY-i7_6CSHC",
        "outputId": "17e5b888-abfe-4e84-ab72-784d41c115b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.move('/content/lda_model_files', '/content/drive/My Drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qs5i0NDmCXnq",
        "outputId": "c43d625d-43bb-4454-90aa-7f621ae3be1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/lda_model_files'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}